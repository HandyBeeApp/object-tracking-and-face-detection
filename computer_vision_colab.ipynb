{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RtKYMSF9Txu"
   },
   "source": [
    "# Computer vision \n",
    "\n",
    "**Aim**: This individual assessment aims to consolidate knowledge base and practical skills to process video and develop object detection and face detection applications. Develop a computer vision system which can track and recognise moving objects and human faces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RLR9TfVoz6p"
   },
   "source": [
    "### Retrieve Task 1 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3EyCqGG-cux"
   },
   "source": [
    "Run the below command to retrieve task1.mp4 and the ground truth files from this public repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_uJOx9M9LSW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "repo = \"object-tracking-and-face-detection\"\n",
    "# Clone repository if not already present\n",
    "if not os.path.exists(repo) and not os.getcwd().endswith(repo):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/HandyBeeApp/object-tracking-and-face-detection.git\"])\n",
    "    os.chdir(repo)\n",
    "elif os.getcwd().endswith(repo):\n",
    "    print(\"Repository already exists and path is correct. Continuing...\")\n",
    "elif os.path.exists(repo):\n",
    "    print(\"Repository already exists, enter repo. Continuing...\")\n",
    "    os.chdir(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3Q8N5QYoz6q"
   },
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glBYp1XQ9LSY"
   },
   "outputs": [],
   "source": [
    "%pip install opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GuecY6mc-8-7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import scipy\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as tvtf\n",
    "from pathlib import Path\n",
    "import sklearn.metrics\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Colors for drawing boxes\n",
    "COLOURS = [\n",
    "    tuple(int(colour_hex.strip(\"#\")[i : i + 2], 16) for i in (0, 2, 4))\n",
    "    for colour_hex in plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "]\n",
    "\n",
    "COLOURS2 = [\n",
    "    tuple(int(c * 180) for c in plt.cm.tab10(i)[:3])  # Max 180 instead of 255\n",
    "    for i in range(10)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-__ZR7e9LSZ"
   },
   "outputs": [],
   "source": [
    "# Class labels that relate to ids from ground_truth_for_task2\n",
    "#and ground_truth_for_task3 files\n",
    "CLASS_LABELS = [\"dog\", \"chair\", \"chair\", \"book\", \"person\", \"face\"]\n",
    "\n",
    "# Frames used to generate ground_truth files\n",
    "EVAL_FRAMES = [0, 30, 40, 64, 150, 210, 270, 295, 306, 330, 360, 420, 449]\n",
    "\n",
    "GT_FACES_TRACKS_IN_VIDEO = 2\n",
    "GT_DIFFERENT_TRACKS_IN_VIDEO = 12\n",
    "\n",
    "\n",
    "# Convulutional Network settings\n",
    "RCNN_WEIGHTS = (\n",
    "    torchvision.models.detection.MaskRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "RCNN_SCORE_THRESHOLD = 0.7  # Min confidence to keep detection\n",
    "\n",
    "# Haar cascade settings\n",
    "SCALE_FACTOR = 1.1\n",
    "MIN_NEIGHBORS = 3\n",
    "MIN_SIZE = (40, 40)\n",
    "MAX_SIZE = (100, 100)\n",
    "\n",
    "## Tracking settings to avoid noise detections\n",
    "# Min length of track to keep detections over 300ms at 30fps\n",
    "MIN_TRACK_LENGTH = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quBFZ2Qaoz6q"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ao5ASWOoz6q"
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTcG5t_s9LSZ"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# UTILITY FUNCTIONS\n",
    "############################################################################\n",
    "\n",
    "def tlbr_to_center(boxes: NDArray) -> List[List[float]]:\n",
    "    \"\"\"Get center point from box corners\"\"\"\n",
    "    points = []\n",
    "    for tlx, tly, brx, bry in boxes:\n",
    "        cx = (tlx + brx) / 2\n",
    "        cy = (tly + bry) / 2\n",
    "        points.append([cx, cy])\n",
    "    return points\n",
    "\n",
    "def bbox_iou_matrix(a: NDArray, b: NDArray) -> NDArray:\n",
    "    \"\"\"\n",
    "    Calculate IoU between all pairs of boxes\n",
    "\n",
    "    Parameters:\n",
    "        a (np.array [N, 4]): N boxes as [tlx, tly, brx, bry]\n",
    "        b (np.array [M, 4]): M boxes as [tlx, tly, brx, bry]\n",
    "\n",
    "    Returns:\n",
    "        np.array [N, M]: IoU for every pair\n",
    "    \"\"\"\n",
    "    a = a[:, None]  # [N, 1, 4]\n",
    "    b = b[None, :]  # [1, M, 4]\n",
    "\n",
    "    tlx_a, tly_a, brx_a, bry_a = [a[..., i] for i in range(4)]\n",
    "    tlx_b, tly_b, brx_b, bry_b = [b[..., i] for i in range(4)]\n",
    "\n",
    "    # Find overlap area\n",
    "    tlx_overlap = np.maximum(tlx_a, tlx_b)\n",
    "    tly_overlap = np.maximum(tly_a, tly_b)\n",
    "    brx_overlap = np.minimum(brx_a, brx_b)\n",
    "    bry_overlap = np.minimum(bry_a, bry_b)\n",
    "\n",
    "    # Clip to zero if no overlap\n",
    "    intersection = (brx_overlap - tlx_overlap).clip(0) * (\n",
    "        bry_overlap - tly_overlap\n",
    "    ).clip(0)\n",
    "\n",
    "    area_a = abs((brx_a - tlx_a) * (bry_a - tly_a))\n",
    "    area_b = abs((brx_b - tlx_b) * (bry_b - tly_b))\n",
    "    union = area_a + area_b - intersection\n",
    "\n",
    "    return intersection / union\n",
    "\n",
    "def load_yolo_ground_truth(fname: str, img_width: int, img_height: int) -> NDArray:\n",
    "    \"\"\"\n",
    "    Load ground truth from YOLO format file\n",
    "\n",
    "    YOLO format: class x_center y_center width height (normalized 0-1)\n",
    "\n",
    "    Args:\n",
    "        fname: Path to .txt file\n",
    "        img_width: Image width\n",
    "        img_height: Image height\n",
    "\n",
    "    Returns:\n",
    "        np.array [N, 5]: [class_id, tlx, tly, brx, bry] in pixels\n",
    "    \"\"\"\n",
    "    if not Path(fname).exists():\n",
    "        return np.array([]).reshape(0, 5)\n",
    "\n",
    "    with open(fname) as f:\n",
    "        rows = []\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                class_id = int(parts[0])\n",
    "                x_center = float(parts[1])\n",
    "                y_center = float(parts[2])\n",
    "                width = float(parts[3])\n",
    "                height = float(parts[4])\n",
    "\n",
    "                # Convert from normalized to pixels\n",
    "                tlx = (x_center - width / 2) * img_width\n",
    "                tly = (y_center - height / 2) * img_height\n",
    "                brx = (x_center + width / 2) * img_width\n",
    "                bry = (y_center + height / 2) * img_height\n",
    "\n",
    "                rows.append([class_id, tlx, tly, brx, bry])\n",
    "\n",
    "    return np.array(rows) if rows else np.array([]).reshape(0, 5)\n",
    "\n",
    "\n",
    "def draw_tracks_on_video(\n",
    "    video_path: str, tracks: List[Dict[str, Union[int, List]]], video_name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Draw tracks on video with consecutive track IDs\n",
    "\n",
    "    Args:\n",
    "        video_path: Input video\n",
    "        tracks: Tracks from do_tracking()\n",
    "        video_name: Output name\n",
    "\n",
    "    Note: Track IDs are automatically consecutive (0, 1, 2, ...)\n",
    "          based on enumerate() iteration order\n",
    "    \"\"\"\n",
    "\n",
    "    output_path = f\"{video_name}2.mp4\"\n",
    "\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    vid_length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(\"m\", \"p\", \"4\", \"v\")\n",
    "    vid_out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for i in range(vid_length):\n",
    "        # Read frame\n",
    "        _, img = vid.read()\n",
    "\n",
    "        # Draw all active tracks (track_id is consecutive: 0, 1, 2, ...)\n",
    "        for track_id, track in enumerate(tracks):\n",
    "            # Check if track is active on this frame\n",
    "            start_frame = track[\"start_frame\"]\n",
    "            inner_idx = i - start_frame\n",
    "\n",
    "            # print(f\"Track ID: {track_id}/{len(tracks)}, Frame: {i}, Inner idx: {inner_idx}\")\n",
    "\n",
    "            if 0 <= inner_idx < len(track[\"boxes\"]):\n",
    "                # Get box\n",
    "                tlx, tly, brx, bry = track[\"boxes\"][inner_idx].astype(np.int32)\n",
    "                label = track[\"labels\"][inner_idx]\n",
    "                score = track[\"scores\"][inner_idx]\n",
    "\n",
    "                # Color by track ID\n",
    "                colour = COLOURS[track_id % len(COLOURS)]\n",
    "\n",
    "                # Draw box\n",
    "                cv2.rectangle(img, (tlx, tly), (brx, bry), color=colour, thickness=2)\n",
    "\n",
    "                # Make label\n",
    "                categories = RCNN_WEIGHTS.meta[\"categories\"]\n",
    "                class_name = categories[label]\n",
    "                text = f\"ID:{track_id} {class_name} {score:.2f}\"\n",
    "\n",
    "                # Text background\n",
    "                (text_width, text_height), _ = cv2.getTextSize(\n",
    "                    text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1\n",
    "                )\n",
    "                cv2.rectangle(\n",
    "                    img,\n",
    "                    (tlx, tly - text_height - 4),\n",
    "                    (tlx + text_width, tly),\n",
    "                    colour,\n",
    "                    -1,\n",
    "                )\n",
    "\n",
    "                # Draw text\n",
    "                cv2.putText(\n",
    "                    img,\n",
    "                    text,\n",
    "                    (tlx, tly - 2),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (255, 255, 255),\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "            # else:\n",
    "            #     print(f\"Track ID: {track_id} skipped on frame {i}\")\n",
    "\n",
    "        vid_out.write(img)\n",
    "        # print(f\"Writing frame {i+1}/{vid_length}\")\n",
    "\n",
    "    vid.release()\n",
    "    vid_out.release()\n",
    "    print(f\"Video saved to {output_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def draw_detections(\n",
    "    img: NDArray,\n",
    "    boxes: NDArray,\n",
    "    colours: List[Tuple[int, int, int]] = COLOURS,\n",
    "    labels: Optional[List[str]] = None,\n",
    "    is_tp: Optional[NDArray] = None,\n",
    "    class_ids: Optional[NDArray] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Draw boxes on image\n",
    "\n",
    "    Args:\n",
    "        img: Image\n",
    "        boxes: np.array [N, 4] [tlx, tly, brx, bry]\n",
    "        colours: Colors\n",
    "        labels: Box labels\n",
    "        is_tp: 1=true positive, 0=false positive\n",
    "        class_ids: Class IDs\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    if not isinstance(boxes, np.ndarray):\n",
    "        boxes = np.array(boxes)\n",
    "\n",
    "    # Check for empty\n",
    "    if boxes.ndim == 0 or boxes.size == 0:\n",
    "        return\n",
    "\n",
    "    # Make 2D\n",
    "    if boxes.ndim == 1:\n",
    "        if len(boxes) < 4:\n",
    "            return\n",
    "        boxes = boxes.reshape(-1, 4)\n",
    "\n",
    "    for i in range(boxes.shape[0]):\n",
    "        box = boxes[i]\n",
    "        tlx, tly, brx, bry = box[0], box[1], box[2], box[3]\n",
    "        # Green=correct, Red=wrong\n",
    "        if is_tp is not None:\n",
    "            color = (0, 255, 0) if is_tp[i] else (255, 0, 0)\n",
    "        else:\n",
    "            color = colours[i % len(colours)]\n",
    "\n",
    "        cv2.rectangle(\n",
    "            img, (int(tlx), int(tly)), (int(brx), int(bry)), color=color, thickness=2\n",
    "        )\n",
    "\n",
    "        # Make label\n",
    "        label_text = \"\"\n",
    "        if labels is not None:\n",
    "            label_text = f\"{labels[i]}\"\n",
    "        elif class_ids is not None and i < len(class_ids):\n",
    "            class_id = int(class_ids[i])\n",
    "            if 0 <= class_id < len(CLASS_LABELS):\n",
    "                label_text = CLASS_LABELS[class_id]\n",
    "\n",
    "        # Add TP/FP\n",
    "        if is_tp is not None and label_text:\n",
    "            label_text += \" (TP)\" if is_tp[i] else \" (FP)\"\n",
    "        elif is_tp is not None:\n",
    "            label_text = \"TP\" if is_tp[i] else \"FP\"\n",
    "\n",
    "        # Draw text\n",
    "        if label_text:\n",
    "            # Background for text\n",
    "            (text_width, text_height), _ = cv2.getTextSize(\n",
    "                label_text, cv2.FONT_HERSHEY_PLAIN, 1.0, 1\n",
    "            )\n",
    "            cv2.rectangle(\n",
    "                img,\n",
    "                (int(tlx), int(tly) - text_height - 4),\n",
    "                (int(tlx) + text_width, int(tly)),\n",
    "                color=color,\n",
    "                thickness=cv2.FILLED,\n",
    "            )\n",
    "            cv2.putText(\n",
    "                img,\n",
    "                label_text,\n",
    "                (int(tlx), int(tly) - 2),\n",
    "                fontFace=cv2.FONT_HERSHEY_PLAIN,\n",
    "                fontScale=1.0,\n",
    "                color=(255, 255, 255),\n",
    "            )\n",
    "\n",
    "\n",
    "def is_face_in_person_box(face_box: List[float], person_box: List[float]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if face is inside person box\n",
    "\n",
    "    Args:\n",
    "        face_box: [x, y, w, h]\n",
    "        person_box: [tlx, tly, brx, bry]\n",
    "\n",
    "    Returns:\n",
    "        True if face center is inside person box\n",
    "    \"\"\"\n",
    "    # Get face center\n",
    "    face_cx = face_box[0] + face_box[2] / 2\n",
    "    face_cy = face_box[1] + face_box[3] / 2\n",
    "\n",
    "    # Check if inside\n",
    "    return (\n",
    "        person_box[0] <= face_cx <= person_box[2]\n",
    "        and person_box[1] <= face_cy <= person_box[3]\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_faces_by_person_boxes(\n",
    "    face_detections: List[Dict[str, Union[List[float], str]]],\n",
    "    person_boxes: List[List[float]],\n",
    ") -> List[Dict[str, Union[List[float], str]]]:\n",
    "    \"\"\"\n",
    "    Keep only faces inside person boxes\n",
    "\n",
    "    Args:\n",
    "        face_detections: Face detections\n",
    "        person_boxes: Person boxes [tlx, tly, brx, bry]\n",
    "\n",
    "    Returns:\n",
    "        Filtered faces\n",
    "    \"\"\"\n",
    "    if len(person_boxes) == 0:\n",
    "        # No person = no faces\n",
    "        return []\n",
    "\n",
    "    filtered_faces = []\n",
    "    for face_det in face_detections:\n",
    "        face_box = face_det[\"box\"]\n",
    "\n",
    "        # Check if inside any person box\n",
    "        for person_box in person_boxes:\n",
    "            if is_face_in_person_box(face_box, person_box):\n",
    "                filtered_faces.append(face_det)\n",
    "                break\n",
    "\n",
    "    return filtered_faces\n",
    "\n",
    "\n",
    "\n",
    "def remove_duplicate_face_detections(\n",
    "    detections: List[Dict[str, Union[List[float], str]]], iou_threshold: float = 0.3\n",
    ") -> List[Dict[str, Union[List[float], str]]]:\n",
    "    \"\"\"\n",
    "    Remove duplicate faces using NMS, keeping larger boxes\n",
    "\n",
    "    Args:\n",
    "        detections: Face detections\n",
    "        iou_threshold: IoU for duplicates\n",
    "\n",
    "    Returns:\n",
    "        Unique faces (larger boxes kept)\n",
    "    \"\"\"\n",
    "    if len(detections) == 0:\n",
    "        return []\n",
    "\n",
    "    # Get boxes\n",
    "    boxes = np.array([d[\"box\"] for d in detections])\n",
    "\n",
    "    # Convert [x, y, w, h] to [x1, y1, x2, y2]\n",
    "    boxes_tlbr = boxes.copy()\n",
    "    boxes_tlbr[:, 2] = boxes[:, 0] + boxes[:, 2]\n",
    "    boxes_tlbr[:, 3] = boxes[:, 1] + boxes[:, 3]\n",
    "\n",
    "    # Calculate box areas\n",
    "    areas = boxes[:, 2] * boxes[:, 3]  # width * height\n",
    "\n",
    "    # Get IoU\n",
    "    iou_matrix = bbox_iou_matrix(boxes_tlbr, boxes_tlbr)\n",
    "\n",
    "    # Track what to keep\n",
    "    keep = []\n",
    "    processed = set()\n",
    "\n",
    "    # Sort by area (largest first)\n",
    "    sorted_indices = np.argsort(areas)[::-1]\n",
    "\n",
    "    for i in sorted_indices:\n",
    "        if i in processed:\n",
    "            continue\n",
    "\n",
    "        keep.append(i)\n",
    "        processed.add(i)\n",
    "\n",
    "        # Mark overlapping smaller boxes as processed\n",
    "        for j in range(len(detections)):\n",
    "            if j != i and j not in processed and iou_matrix[i, j] > iou_threshold:\n",
    "                processed.add(j)\n",
    "\n",
    "    return [detections[i] for i in keep]\n",
    "\n",
    "\n",
    "\n",
    "def export_specific_frames(\n",
    "    video_path: str, output_dir: str, frame_numbers: List[int], prefix: str = \"frame\"\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Save specific frames from video\n",
    "\n",
    "    Args:\n",
    "        video_path: Video file\n",
    "        output_dir: Where to save\n",
    "        frame_numbers: Frames to export\n",
    "        prefix: Filename prefix\n",
    "\n",
    "    Returns:\n",
    "        List of saved paths\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # Make folder\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    vid_length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(vid.get(cv2.CAP_PROP_FPS)) + 1\n",
    "\n",
    "    # Clean frame list\n",
    "    frame_numbers = sorted(set(frame_numbers))\n",
    "\n",
    "    # Remove invalid frames\n",
    "    valid_frames = [f for f in frame_numbers if 0 <= f < vid_length]\n",
    "    if len(valid_frames) < len(frame_numbers):\n",
    "        invalid = set(frame_numbers) - set(valid_frames)\n",
    "        print(f\"Warning: Skipping invalid frames: {invalid}\")\n",
    "\n",
    "    exported_frames = []\n",
    "\n",
    "    print(\"Video:\")\n",
    "    print(f\"  Frames: {vid_length}\")\n",
    "    print(f\"  FPS: {fps}\")\n",
    "    print(f\"  Exporting {len(valid_frames)} frames\")\n",
    "    print(f\"  Numbers: {valid_frames}\")\n",
    "\n",
    "    # Save each frame\n",
    "    for frame_num in valid_frames:\n",
    "        # Get frame\n",
    "        vid.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = vid.read()\n",
    "\n",
    "        if not ret:\n",
    "            print(f\"Warning: Can't read frame {frame_num}\")\n",
    "            continue\n",
    "\n",
    "        # Make filename\n",
    "        filename = f\"{prefix}_{frame_num}.jpg\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        # Save\n",
    "        cv2.imwrite(filepath, frame)\n",
    "        exported_frames.append(filepath)\n",
    "\n",
    "        print(f\"Exported frame {frame_num} -> {filename}\")\n",
    "\n",
    "    vid.release()\n",
    "    print(f\"\\nSaved {len(exported_frames)} frames to {output_dir}\")\n",
    "\n",
    "    return exported_frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NV3zTkIQoz6r"
   },
   "source": [
    "### Detection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uN5Vo6jz9LSa"
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# DETECTION FUNCTIONS\n",
    "############################################################################\n",
    "\n",
    "def preprocess_image(image: NDArray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Get image ready for MaskRCNN\n",
    "    Torchvision handles the transforms\n",
    "    \"\"\"\n",
    "    image = tvtf.to_tensor(image)\n",
    "\n",
    "    # Get transforms from weights\n",
    "    preprocess = RCNN_WEIGHTS.transforms()\n",
    "\n",
    "    # Apply transforms and add batch dimension\n",
    "    image = preprocess(image).unsqueeze(dim=0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_detections(\n",
    "    maskrcnn: torch.nn.Module, vid: cv2.VideoCapture\n",
    ") -> List[Dict[str, NDArray]]:\n",
    "    \"\"\"\n",
    "    Run model on all video frames\n",
    "    Returns boxes, labels, and scores for each frame\n",
    "    \"\"\"\n",
    "    vid_length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    initial_frame = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "    all_detections = []\n",
    "    for i in range(vid_length):\n",
    "        _, img = vid.read()\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Run model\n",
    "        with torch.no_grad():\n",
    "            result = maskrcnn(preprocess_image(img))[0]\n",
    "\n",
    "        # Keep only high-confidence detections\n",
    "        mask = result[\"scores\"] > RCNN_SCORE_THRESHOLD\n",
    "\n",
    "        boxes = result[\"boxes\"][mask].detach().cpu().numpy()\n",
    "        labels = result[\"labels\"][mask].detach().cpu().numpy()\n",
    "        scores = result[\"scores\"][mask].detach().cpu().numpy()\n",
    "\n",
    "        all_detections.append({\"boxes\": boxes, \"labels\": labels, \"scores\": scores})\n",
    "\n",
    "        # print(f\"Processing frame {i+1}/{vid_length}\")\n",
    "    print(f\"Processed {vid_length} frames\")\n",
    "\n",
    "    vid.set(cv2.CAP_PROP_POS_FRAMES, initial_frame)\n",
    "    return all_detections\n",
    "\n",
    "\n",
    "def object_detections(\n",
    "    video_path: str, video_name: str = \"task1\"\n",
    ") -> List[Dict[str, NDArray]]:\n",
    "    \"\"\"\n",
    "    Run MaskRCNN on all frames\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    detection_file = f\"{video_name}1.th\"\n",
    "\n",
    "    print(\"Loading MaskRCNN...\")\n",
    "    # Use default weights\n",
    "    maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn(\n",
    "        weights=\"DEFAULT\", pretrained=True\n",
    "    )\n",
    "    maskrcnn.eval()\n",
    "\n",
    "    print(\"\\nRunning detections...\")\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    all_detections = get_detections(maskrcnn, vid)\n",
    "    vid.release()\n",
    "\n",
    "    return all_detections\n",
    "\n",
    "\n",
    "def associate_object_tracking(\n",
    "    track_boxes: NDArray, det_boxes: NDArray, method: str = \"iou\"\n",
    ") -> Tuple[NDArray, NDArray]:\n",
    "    \"\"\"\n",
    "    Match tracks to detections\n",
    "\n",
    "    Args:\n",
    "        track_boxes: Last box of each track [N, 4] [tlx, tly, brx, bry]\n",
    "        det_boxes: New detections [M, 4] [tlx, tly, brx, bry]\n",
    "        method: 'iou' or 'distance'\n",
    "\n",
    "    Returns:\n",
    "        Matching indices from Hungarian algorithm\n",
    "    \"\"\"\n",
    "    if method == \"iou\":\n",
    "        # IoU-based matching\n",
    "        cost = 1 - bbox_iou_matrix(track_boxes, det_boxes)\n",
    "    else:\n",
    "        # Distance-based matching\n",
    "        track_points = np.array(tlbr_to_center(track_boxes))\n",
    "        det_points = np.array(tlbr_to_center(det_boxes))\n",
    "        cost = np.linalg.norm(track_points[:, None] - det_points[None], axis=-1)\n",
    "\n",
    "    return scipy.optimize.linear_sum_assignment(cost)\n",
    "\n",
    "\n",
    "def filter_short_tracks(\n",
    "    tracks: List[Dict[str, Union[int, List]]], min_length: int = MIN_TRACK_LENGTH\n",
    ") -> List[Dict[str, Union[int, List]]]:\n",
    "    \"\"\"\n",
    "    Remove tracks that appear in fewer than min_length frames.\n",
    "    This helps remove noise from detections that only appear once.\n",
    "\n",
    "    Args:\n",
    "        tracks: List of track dictionaries with 'boxes' or 'points'\n",
    "        min_length: Minimum number of frames a track must appear in\n",
    "\n",
    "    Returns:\n",
    "        Filtered list of tracks\n",
    "    \"\"\"\n",
    "    filtered_tracks = []\n",
    "    for track in tracks:\n",
    "        # Check the length of the track\n",
    "        if 'boxes' in track:\n",
    "            track_length = len(track['boxes'])\n",
    "        elif 'points' in track:\n",
    "            track_length = len(track['points'])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        # Only keep tracks that appear in at least min_length frames\n",
    "        if track_length >= min_length:\n",
    "            filtered_tracks.append(track)\n",
    "\n",
    "    return filtered_tracks\n",
    "\n",
    "\n",
    "def do_tracking(\n",
    "    detections: List[Dict[str, NDArray]], association_method: str = \"iou\"\n",
    ") -> List[Dict[str, Union[int, List]]]:\n",
    "    \"\"\"\n",
    "    Track objects across frames\n",
    "\n",
    "    Args:\n",
    "        detections: Detections with boxes, labels, scores\n",
    "        association_method: 'iou' or 'centroid_distance'\n",
    "\n",
    "    Returns:\n",
    "        List of tracks with start_frame, boxes, labels, scores\n",
    "    \"\"\"\n",
    "    open_tracks = []\n",
    "    closed_tracks = []\n",
    "\n",
    "    for i, det in enumerate(detections):\n",
    "        det_boxes = det[\"boxes\"]\n",
    "        det_labels = det[\"labels\"]\n",
    "        det_scores = det[\"scores\"]\n",
    "\n",
    "        track_indices = det_indices = []\n",
    "\n",
    "        if i > 0 and len(open_tracks) > 0 and len(det_boxes) > 0:\n",
    "            # Match detections to existing tracks\n",
    "            track_boxes = np.array([track[\"boxes\"][-1] for track in open_tracks])\n",
    "            track_indices, det_indices = associate_object_tracking(\n",
    "                track_boxes, det_boxes, method=association_method\n",
    "            )\n",
    "\n",
    "        # Update matched tracks\n",
    "        for track_idx, det_idx in zip(track_indices, det_indices):\n",
    "            open_tracks[track_idx][\"boxes\"].append(det_boxes[det_idx])\n",
    "            open_tracks[track_idx][\"labels\"].append(det_labels[det_idx])\n",
    "            open_tracks[track_idx][\"scores\"].append(det_scores[det_idx])\n",
    "\n",
    "        # Close tracks that lost matching\n",
    "        lost_indices = set(range(len(open_tracks))) - set(track_indices)\n",
    "        for lost_idx in sorted(lost_indices, reverse=True):\n",
    "            closed_tracks.append(open_tracks.pop(lost_idx))\n",
    "\n",
    "        # Start new tracks\n",
    "        new_indices = set(range(len(det_boxes))) - set(det_indices)\n",
    "        for new_idx in new_indices:\n",
    "            open_tracks.append(\n",
    "                {\n",
    "                    \"start_frame\": i,\n",
    "                    \"boxes\": [det_boxes[new_idx]],\n",
    "                    \"labels\": [det_labels[new_idx]],\n",
    "                    \"scores\": [det_scores[new_idx]],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    all_tracks = closed_tracks + open_tracks\n",
    "\n",
    "    # Filter out short tracks (noise)\n",
    "    if MIN_TRACK_LENGTH > 1:\n",
    "        filtered_tracks = filter_short_tracks(all_tracks)\n",
    "        print(f\"Filtered out {len(all_tracks) - len(filtered_tracks)} short tracks (noise)\")\n",
    "        all_tracks = filtered_tracks\n",
    "\n",
    "    return all_tracks\n",
    "\n",
    "\n",
    "def do_face_tracking(\n",
    "    face_detections_per_frame: List[List[Dict[str, Union[List[float], str]]]],\n",
    "    association_method: str = \"iou\"\n",
    ") -> List[Dict[str, Union[int, List]]]:\n",
    "    \"\"\"\n",
    "    Track faces across frames (similar to object tracking)\n",
    "\n",
    "    Args:\n",
    "        face_detections_per_frame: List of face detections for each frame\n",
    "                                   Each detection has 'box' [x, y, w, h] and 'cascade_name'\n",
    "        association_method: 'iou' or 'centroid_distance'\n",
    "\n",
    "    Returns:\n",
    "        List of face tracks with start_frame, boxes, cascade_names\n",
    "    \"\"\"\n",
    "    open_tracks = []\n",
    "    closed_tracks = []\n",
    "\n",
    "    for i, face_detections in enumerate(face_detections_per_frame):\n",
    "        # Convert face boxes from [x, y, w, h] to [tlx, tly, brx, bry]\n",
    "        det_boxes = []\n",
    "        cascade_names = []\n",
    "\n",
    "        for det in face_detections:\n",
    "            x, y, w, h = det[\"box\"]\n",
    "            det_boxes.append([x, y, x + w, y + h])\n",
    "            cascade_names.append(det[\"cascade_name\"])\n",
    "\n",
    "        det_boxes = np.array(det_boxes) if det_boxes else np.array([]).reshape(0, 4)\n",
    "\n",
    "        track_indices = det_indices = []\n",
    "\n",
    "        if i > 0 and len(open_tracks) > 0 and len(det_boxes) > 0:\n",
    "            # Match detections to existing tracks\n",
    "            track_boxes = np.array([track[\"boxes\"][-1] for track in open_tracks])\n",
    "            track_indices, det_indices = associate_object_tracking(\n",
    "                track_boxes, det_boxes, method=association_method\n",
    "            )\n",
    "\n",
    "        # Update matched tracks\n",
    "        for track_idx, det_idx in zip(track_indices, det_indices):\n",
    "            open_tracks[track_idx][\"boxes\"].append(det_boxes[det_idx])\n",
    "            open_tracks[track_idx][\"cascade_names\"].append(cascade_names[det_idx])\n",
    "\n",
    "        # Close tracks that lost matching\n",
    "        lost_indices = set(range(len(open_tracks))) - set(track_indices)\n",
    "        for lost_idx in sorted(lost_indices, reverse=True):\n",
    "            closed_tracks.append(open_tracks.pop(lost_idx))\n",
    "\n",
    "        # Start new tracks\n",
    "        new_indices = set(range(len(det_boxes))) - set(det_indices)\n",
    "        for new_idx in new_indices:\n",
    "            open_tracks.append(\n",
    "                {\n",
    "                    \"start_frame\": i,\n",
    "                    \"boxes\": [det_boxes[new_idx]],\n",
    "                    \"cascade_names\": [cascade_names[new_idx]],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    all_tracks = closed_tracks + open_tracks\n",
    "\n",
    "    # Filter out short tracks (noise)\n",
    "    if MIN_TRACK_LENGTH > 1:\n",
    "        filtered_tracks = filter_short_tracks(all_tracks)\n",
    "        print(f\"Filtered out {len(all_tracks) - len(filtered_tracks)} short face tracks (noise)\")\n",
    "        all_tracks = filtered_tracks\n",
    "\n",
    "    return all_tracks\n",
    "\n",
    "\n",
    "def facial_recognition_with_cascades(\n",
    "    frame: NDArray, person_boxes: Optional[List[List[float]]] = None\n",
    ") -> List[Dict[str, Union[List[float], str]]]:\n",
    "    \"\"\"\n",
    "    Detect faces using Haar cascades\n",
    "\n",
    "    Args:\n",
    "        frame: BGR image\n",
    "        person_boxes: Person boxes [tlx, tly, brx, bry]\n",
    "                     Only return faces inside these\n",
    "\n",
    "    Returns:\n",
    "        List with box [x, y, w, h] and cascade_name\n",
    "    \"\"\"\n",
    "    # Convert to gray\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Removed as histogram equalization can worsen detection, same thing happened in assessment 1\n",
    "    # And I was penalised for not using it even though it wasn't a requirement :(\n",
    "    # gray = cv2.equalizeHist(gray)\n",
    "\n",
    "    # Load cascades\n",
    "    cascades = {\n",
    "        \"frontalface_default\": cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "        ),\n",
    "        \"profileface\": cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + \"haarcascade_profileface.xml\"\n",
    "        ),\n",
    "        \"frontalface_alt\": cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + \"haarcascade_frontalface_alt.xml\"\n",
    "        ),\n",
    "         \"frontalface_alt2\": cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + \"haarcascade_frontalface_alt2.xml\"\n",
    "        ),\n",
    "         \"frontalface_alt3\": cv2.CascadeClassifier(\n",
    "            cv2.data.haarcascades + \"haarcascade_frontalface_alt_tree.xml\"\n",
    "        ),\n",
    "\n",
    "    }\n",
    "\n",
    "    all_detections = []\n",
    "\n",
    "    # Run each cascade\n",
    "    for cascade_name, cascade in cascades.items():\n",
    "        faces = cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=SCALE_FACTOR,\n",
    "            minNeighbors=MIN_NEIGHBORS,\n",
    "            minSize=MIN_SIZE,\n",
    "            maxSize=MAX_SIZE\n",
    "        )\n",
    "\n",
    "        for x, y, w, h in faces:\n",
    "            all_detections.append({\"box\": [x, y, w, h], \"cascade_name\": cascade_name})\n",
    "\n",
    "    # Remove duplicates\n",
    "    keep_detections = remove_duplicate_face_detections(all_detections)\n",
    "\n",
    "    # Detect how many faces were removed\n",
    "    # if(len(all_detections) - len(keep_detections) > 0):\n",
    "    #     print(f\"Removed {len(all_detections) - len(keep_detections)} duplicate face detections out of {len(all_detections)} \")\n",
    "    all_detections = keep_detections\n",
    "\n",
    "    # Filter by person boxes\n",
    "    if person_boxes is not None and len(person_boxes) > 0:\n",
    "        all_detections = filter_faces_by_person_boxes(all_detections, person_boxes)\n",
    "\n",
    "    return all_detections\n",
    "\n",
    "def add_haar_detections_to_video(\n",
    "    input_video_path: str,\n",
    "    output_video_path: str,\n",
    "    all_detections: Optional[List[Dict[str, NDArray]]] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Add face detection and tracking to video\n",
    "\n",
    "    Args:\n",
    "        input_video_path: Input video\n",
    "        output_video_path: Output video\n",
    "        all_detections: Object detections to get person boxes\n",
    "    \"\"\"\n",
    "    vid = cv2.VideoCapture(input_video_path)\n",
    "    vid_length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(vid.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # First pass: collect all face detections\n",
    "    print(\"Detecting faces in all frames...\")\n",
    "    all_face_detections = []\n",
    "\n",
    "    for i in range(vid_length):\n",
    "        # Read frame\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Get person boxes for this frame\n",
    "        person_boxes = []\n",
    "        if all_detections is not None and i < len(all_detections):\n",
    "            det = all_detections[i]\n",
    "            # COCO class 1 = person\n",
    "            for idx, label in enumerate(det[\"labels\"]):\n",
    "                if label == 1:\n",
    "                    person_boxes.append(det[\"boxes\"][idx])\n",
    "\n",
    "        # Detect faces\n",
    "        face_detections = facial_recognition_with_cascades(\n",
    "            frame, person_boxes=person_boxes if person_boxes else None\n",
    "        )\n",
    "\n",
    "        all_face_detections.append(face_detections)\n",
    "\n",
    "    vid.release()\n",
    "    print(f\"Detected faces in {vid_length} frames\")\n",
    "\n",
    "    # Track faces across frames\n",
    "    print(\"Tracking faces...\")\n",
    "    face_tracks = do_face_tracking(all_face_detections, association_method=\"centroid_distance\")\n",
    "    print(f\"Created {len(face_tracks)} face tracks\")\n",
    "\n",
    "    # Second pass: draw tracked faces on video\n",
    "    print(\"Drawing tracked faces on video...\")\n",
    "    vid = cv2.VideoCapture(input_video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(\"m\", \"p\", \"4\", \"v\")\n",
    "    vid_out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    for i in range(vid_length):\n",
    "        # Read frame\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Draw all active face tracks\n",
    "        for track_id, track in enumerate(face_tracks):\n",
    "            # Check if track is active on this frame\n",
    "            start_frame = track[\"start_frame\"]\n",
    "            inner_idx = i - start_frame\n",
    "\n",
    "            if 0 <= inner_idx < len(track[\"boxes\"]):\n",
    "                # Get box [tlx, tly, brx, bry]\n",
    "                tlx, tly, brx, bry = track[\"boxes\"][inner_idx].astype(np.int32)\n",
    "                cascade_name = track[\"cascade_names\"][inner_idx]\n",
    "\n",
    "                # Color by track ID\n",
    "                colour = COLOURS[track_id % len(COLOURS)]\n",
    "\n",
    "                # Draw box\n",
    "                cv2.rectangle(frame, (tlx, tly), (brx, bry), color=colour, thickness=2)\n",
    "\n",
    "                # Make label\n",
    "                text = f\"Face ID:{track_id}\"\n",
    "                (text_width, text_height), _ = cv2.getTextSize(\n",
    "                    text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1\n",
    "                )\n",
    "\n",
    "                # Background\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (tlx, tly - text_height - 4),\n",
    "                    (tlx + text_width, tly),\n",
    "                    colour,\n",
    "                    -1,\n",
    "                )\n",
    "\n",
    "                # Text\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    text,\n",
    "                    (tlx, tly - 2),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (255, 255, 255),\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "        vid_out.write(frame)\n",
    "\n",
    "    vid.release()\n",
    "    vid_out.release()\n",
    "    print(f\"Face tracking video saved to {output_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKaFNIA4oz6s"
   },
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYgeQmma9LSa"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def determine_true_positives(\n",
    "    pred_boxes: NDArray, gt_boxes: NDArray, iou_thresh: float = 0.5\n",
    ") -> NDArray:\n",
    "    \"\"\"\n",
    "    Check which predictions match ground truth\n",
    "\n",
    "    Args:\n",
    "        pred_boxes: np.array [N, 4] predictions [tlx, tly, brx, bry]\n",
    "        gt_boxes: np.array [M, 4] ground truth [tlx, tly, brx, bry]\n",
    "        iou_thresh: IoU threshold (default: 0.5)\n",
    "\n",
    "    Returns:\n",
    "        np.array [N]: 1 for true positive, 0 for false positive\n",
    "    \"\"\"\n",
    "    # Make sure we have numpy arrays\n",
    "    if not isinstance(pred_boxes, np.ndarray):\n",
    "        pred_boxes = np.array(pred_boxes)\n",
    "    if not isinstance(gt_boxes, np.ndarray):\n",
    "        gt_boxes = np.array(gt_boxes)\n",
    "\n",
    "    # Make sure arrays are 2D\n",
    "    if pred_boxes.ndim == 0 or (pred_boxes.ndim == 1 and len(pred_boxes) == 0):\n",
    "        return np.array([])\n",
    "    if pred_boxes.ndim == 1:\n",
    "        pred_boxes = pred_boxes.reshape(-1, 4)\n",
    "\n",
    "    if gt_boxes.ndim == 0 or (gt_boxes.ndim == 1 and len(gt_boxes) == 0):\n",
    "        # No ground truth = all predictions are wrong\n",
    "        return (\n",
    "            np.zeros(pred_boxes.shape[0], dtype=int)\n",
    "            if pred_boxes.ndim > 0\n",
    "            else np.array([])\n",
    "        )\n",
    "    if gt_boxes.ndim == 1:\n",
    "        gt_boxes = gt_boxes.reshape(-1, 4)\n",
    "\n",
    "    # Check for empty arrays\n",
    "    if pred_boxes.size == 0 or pred_boxes.shape[0] == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    if gt_boxes.size == 0 or gt_boxes.shape[0] == 0:\n",
    "        # No ground truth = all predictions are wrong\n",
    "        return np.zeros(pred_boxes.shape[0], dtype=int)\n",
    "\n",
    "    # Get IoU for all pairs\n",
    "    iou_matrix = bbox_iou_matrix(pred_boxes, gt_boxes)\n",
    "\n",
    "    # Check if prediction matches any ground truth\n",
    "    max_ious = iou_matrix.max(axis=1)\n",
    "    is_tp = (max_ious >= iou_thresh).astype(int)\n",
    "\n",
    "    return is_tp\n",
    "\n",
    "\n",
    "def calculate_metrics(tp: int, fp: int, fn: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, F1 using sklearn\n",
    "\n",
    "    Args:\n",
    "        tp: True positives\n",
    "        fp: False positives\n",
    "        fn: False negatives\n",
    "\n",
    "    Returns:\n",
    "        dict: precision, recall, f1, accuracy, tp, fp, fn, tn\n",
    "    \"\"\"\n",
    "    # Convert to int\n",
    "    tp = int(tp)\n",
    "    fp = int(fp)\n",
    "    fn = int(fn)\n",
    "\n",
    "    # Build arrays for sklearn\n",
    "    total_samples = tp + fp + fn\n",
    "\n",
    "    if total_samples == 0:\n",
    "        # Nothing detected, nothing expected = correct\n",
    "        tn = 1\n",
    "        return {\n",
    "            \"precision\": 0.0,  # Undefined, default to 0\n",
    "            \"recall\": 0.0,  # Undefined, default to 0\n",
    "            \"f1\": 0.0,  # Undefined, default to 0\n",
    "            \"accuracy\": 1.0,  # Correctly predicted nothing\n",
    "            \"tp\": 0,\n",
    "            \"fp\": 0,\n",
    "            \"fn\": 0,\n",
    "            \"tn\": tn,\n",
    "        }\n",
    "\n",
    "    # Build prediction and truth arrays\n",
    "    y_pred = np.concatenate(\n",
    "        [\n",
    "            np.ones(tp + fp),  # What we predicted\n",
    "            np.zeros(fn),  # What we missed\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y_true = np.concatenate(\n",
    "        [\n",
    "            np.ones(tp),  # Correct predictions\n",
    "            np.zeros(fp),  # Wrong predictions\n",
    "            np.ones(fn),  # Missed ones\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Use sklearn\n",
    "    precision = sklearn.metrics.precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = sklearn.metrics.recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = sklearn.metrics.f1_score(y_true, y_pred, zero_division=0)\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # We don't track TN in object detection\n",
    "    tn = 0\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"tp\": int(tp),\n",
    "        \"fp\": int(fp),\n",
    "        \"fn\": int(fn),\n",
    "        \"tn\": int(tn),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_track_quality(\n",
    "    tracks: List[Dict[str, Union[int, List]]],\n",
    "    fps: int = 30\n",
    ") -> Dict[str, Union[int, float]]:\n",
    "    \"\"\"\n",
    "    Evaluate tracking quality based on track statistics\n",
    "\n",
    "    Args:\n",
    "        tracks: List of tracks from do_tracking() or do_face_tracking()\n",
    "        fps: Video frames per second (default: 30)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with track quality metrics\n",
    "    \"\"\"\n",
    "    if not tracks:\n",
    "        return {\n",
    "            'total_tracks': 0,\n",
    "            'avg_track_length_frames': 0,\n",
    "            'avg_track_length_seconds': 0.0,\n",
    "            'median_track_length_frames': 0,\n",
    "            'median_track_length_seconds': 0.0,\n",
    "            'max_track_length_frames': 0,\n",
    "            'max_track_length_seconds': 0.0,\n",
    "            'min_track_length_frames': 0,\n",
    "            'min_track_length_seconds': 0.0,\n",
    "            'short_tracks': 0,  # < 1 second\n",
    "            'medium_tracks': 0,  # 1-5 seconds\n",
    "            'long_tracks': 0,   # > 5 seconds\n",
    "        }\n",
    "\n",
    "    # Get track lengths\n",
    "    track_lengths = [len(t['boxes']) for t in tracks]\n",
    "\n",
    "    # Convert to seconds\n",
    "    track_lengths_sec = [l / fps for l in track_lengths]\n",
    "\n",
    "    # Calculate statistics\n",
    "    avg_length = np.mean(track_lengths)\n",
    "    median_length = np.median(track_lengths)\n",
    "    max_length = np.max(track_lengths)\n",
    "    min_length = np.min(track_lengths)\n",
    "\n",
    "    # Categorize tracks by duration\n",
    "    short_tracks = sum(1 for l in track_lengths if l < fps)  # < 1 sec\n",
    "    medium_tracks = sum(1 for l in track_lengths if fps <= l <= 5 * fps)  # 1-5 sec\n",
    "    long_tracks = sum(1 for l in track_lengths if l > 5 * fps)  # > 5 sec\n",
    "\n",
    "    return {\n",
    "        'total_tracks': len(tracks),\n",
    "        'avg_track_length_frames': float(avg_length),\n",
    "        'avg_track_length_seconds': float(avg_length / fps),\n",
    "        'median_track_length_frames': float(median_length),\n",
    "        'median_track_length_seconds': float(median_length / fps),\n",
    "        'max_track_length_frames': int(max_length),\n",
    "        'max_track_length_seconds': float(max_length / fps),\n",
    "        'min_track_length_frames': int(min_length),\n",
    "        'min_track_length_seconds': float(min_length / fps),\n",
    "        'short_tracks': short_tracks,\n",
    "        'medium_tracks': medium_tracks,\n",
    "        'long_tracks': long_tracks,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_tracking_accuracy(\n",
    "    detected_tracks: int,\n",
    "    ground_truth_tracks: int\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate tracking accuracy using sklearn metrics\n",
    "\n",
    "    Treats tracking as a binary classification problem:\n",
    "    - Each ground truth track should result in exactly 1 detected track\n",
    "    - TP: Correctly detected tracks (min of detected and GT)\n",
    "    - FP: Extra detected tracks (over-tracking)\n",
    "    - FN: Missed tracks (under-tracking)\n",
    "\n",
    "    Args:\n",
    "        detected_tracks: Number of tracks detected by the algorithm\n",
    "        ground_truth_tracks: Ground truth number of unique objects/faces\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with precision, recall, f1, accuracy (sklearn formulas)\n",
    "    \"\"\"\n",
    "    # Calculate confusion matrix elements\n",
    "    tp = min(detected_tracks, ground_truth_tracks)\n",
    "    fp = max(0, detected_tracks - ground_truth_tracks)  # Over-tracking\n",
    "    fn = max(0, ground_truth_tracks - detected_tracks)  # Under-tracking\n",
    "\n",
    "    # Use sklearn's calculate_metrics function\n",
    "    return calculate_metrics(tp, fp, fn)\n",
    "\n",
    "\n",
    "def visualize_iou_comparison(\n",
    "    video_path: str,\n",
    "    frame_num: int,\n",
    "    pred_boxes: NDArray,\n",
    "    gt_boxes: NDArray,\n",
    "    is_tp: NDArray,\n",
    "    output_dir: str,\n",
    "    prefix: str = \"frame\",\n",
    "    pred_class_ids: Optional[NDArray] = None,\n",
    "    gt_class_ids: Optional[NDArray] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save side-by-side comparison of predictions vs ground truth\n",
    "\n",
    "    Args:\n",
    "        video_path: Video file\n",
    "        frame_num: Frame to show\n",
    "        pred_boxes: Predictions [N, 4] [tlx, tly, brx, bry]\n",
    "        gt_boxes: Ground truth [M, 4] [tlx, tly, brx, bry]\n",
    "        is_tp: True positive flags\n",
    "        output_dir: Where to save\n",
    "        prefix: Filename prefix\n",
    "        pred_class_ids: Class IDs for predictions\n",
    "        gt_class_ids: Class IDs for ground truth\n",
    "    \"\"\"\n",
    "    # Get frame\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ret:\n",
    "        print(f\"Could not read frame {frame_num}\")\n",
    "        return\n",
    "\n",
    "    # Convert to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Make side-by-side image\n",
    "    h, w = frame_rgb.shape[:2]\n",
    "    combined_img = np.zeros((h, w * 2, 3), dtype=np.uint8)\n",
    "\n",
    "    # Left: Ground truth\n",
    "    gt_img = frame_rgb.copy()\n",
    "    if len(gt_boxes) > 0:\n",
    "        draw_detections(\n",
    "            gt_img, gt_boxes, labels=[\"GT\"] * len(gt_boxes), class_ids=gt_class_ids\n",
    "        )\n",
    "    cv2.putText(\n",
    "        gt_img,\n",
    "        \"Ground Truth\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        1.0,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "    )\n",
    "    combined_img[:, :w] = gt_img\n",
    "\n",
    "    # Right: Predictions\n",
    "    pred_img = frame_rgb.copy()\n",
    "    if len(pred_boxes) > 0:\n",
    "        draw_detections(pred_img, pred_boxes, is_tp=is_tp, class_ids=pred_class_ids)\n",
    "    cv2.putText(\n",
    "        pred_img,\n",
    "        \"Predictions (Green=TP, Red=FP)\",\n",
    "        (10, 30),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.6,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "    )\n",
    "    combined_img[:, w:] = pred_img\n",
    "\n",
    "    # Save\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(\n",
    "        output_dir, f\"{prefix}_{frame_num:06d}_iou_comparison.jpg\"\n",
    "    )\n",
    "    combined_img_bgr = cv2.cvtColor(combined_img, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(output_path, combined_img_bgr)\n",
    "\n",
    "\n",
    "def evaluate_detections_for_frame(\n",
    "    pred_boxes: NDArray,\n",
    "    gt_file: str,\n",
    "    img_width: int,\n",
    "    img_height: int,\n",
    "    iou_thresh: float = 0.5,\n",
    "    pred_class_ids: Optional[NDArray] = None,\n",
    ") -> Tuple[Dict[str, float], NDArray, NDArray, NDArray]:\n",
    "    \"\"\"\n",
    "    Check predictions for one frame\n",
    "\n",
    "    Args:\n",
    "        pred_boxes: np.array [N, 4] [tlx, tly, brx, bry]\n",
    "        gt_file: Ground truth file\n",
    "        img_width: Image width\n",
    "        img_height: Image height\n",
    "        iou_thresh: IoU threshold (default: 0.5)\n",
    "        pred_class_ids: Class IDs\n",
    "\n",
    "    Returns:\n",
    "        (metrics, is_tp, gt_boxes, gt_class_ids)\n",
    "    \"\"\"\n",
    "    # Load ground truth\n",
    "    gt_data = load_yolo_ground_truth(gt_file, img_width, img_height)\n",
    "\n",
    "    if len(gt_data) == 0:\n",
    "        # No ground truth = all wrong\n",
    "        is_tp = (\n",
    "            np.zeros(len(pred_boxes), dtype=int)\n",
    "            if len(pred_boxes) > 0\n",
    "            else np.array([])\n",
    "        )\n",
    "        return (\n",
    "            calculate_metrics(0, len(pred_boxes), 0),\n",
    "            is_tp,\n",
    "            np.array([]).reshape(0, 4),\n",
    "            np.array([]),\n",
    "        )\n",
    "\n",
    "    gt_boxes = gt_data[:, 1:5]  # Box coords\n",
    "    gt_class_ids = gt_data[:, 0].astype(int)  # Class IDs\n",
    "\n",
    "    # Check what matched\n",
    "    is_tp = determine_true_positives(pred_boxes, gt_boxes, iou_thresh)\n",
    "\n",
    "    tp = is_tp.sum()\n",
    "    fp = len(pred_boxes) - tp\n",
    "    fn = max(0, len(gt_boxes) - tp)  # Missed boxes\n",
    "\n",
    "    return calculate_metrics(tp, fp, fn), is_tp, gt_boxes, gt_class_ids\n",
    "\n",
    "\n",
    "def evaluate_task2_detections(\n",
    "    all_detections: List[Dict[str, NDArray]],\n",
    "    video_name: str,\n",
    "    gt_dir: str = \"ground_truth_for_task2\",\n",
    "    iou_thresh: float = 0.5,\n",
    "    eval_frames: Optional[List[int]] = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Check object detection results (Task 2)\n",
    "\n",
    "    Args:\n",
    "        all_detections: All detections from video\n",
    "        video_name: Video name (e.g., \"task\")\n",
    "        gt_dir: Ground truth folder\n",
    "        iou_thresh: IoU threshold (default: 0.5)\n",
    "        eval_frames: Frames to check (default: EVAL_FRAMES)\n",
    "\n",
    "    Returns:\n",
    "        Overall metrics\n",
    "    \"\"\"\n",
    "    # Get video size\n",
    "    video_path = f\"./{video_name}1.mp4\"\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    img_width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    img_height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    vid.release()\n",
    "\n",
    "    # Use default frames if not provided\n",
    "    if eval_frames is None:\n",
    "        eval_frames = EVAL_FRAMES\n",
    "\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    frame_metrics = []\n",
    "\n",
    "    print(\"OBJECT DETECTION EVALUATION\")\n",
    "\n",
    "\n",
    "    # Make output folder\n",
    "    vis_dir = f\"evaluation_results/{video_name.strip('_')}_task2\"\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    for frame_num in eval_frames:\n",
    "        if frame_num >= len(all_detections):\n",
    "            continue\n",
    "\n",
    "        # Get predictions\n",
    "        det = all_detections[frame_num]\n",
    "        pred_boxes = det[\"boxes\"]\n",
    "        pred_labels = det[\"labels\"]\n",
    "\n",
    "        # GT file\n",
    "        gt_file = Path(gt_dir) / f\"task1_frame_{frame_num}.txt\"\n",
    "\n",
    "        if not gt_file.exists():\n",
    "            print(f\"Warning: No GT for frame {frame_num}\")\n",
    "            continue\n",
    "\n",
    "        # Evaluate frame\n",
    "        metrics, is_tp, gt_boxes, gt_class_ids = evaluate_detections_for_frame(\n",
    "            pred_boxes,\n",
    "            gt_file,\n",
    "            img_width,\n",
    "            img_height,\n",
    "            iou_thresh,\n",
    "            pred_class_ids=pred_labels,\n",
    "        )\n",
    "\n",
    "        total_tp += metrics[\"tp\"]\n",
    "        total_fp += metrics[\"fp\"]\n",
    "        total_fn += metrics[\"fn\"]\n",
    "\n",
    "        frame_metrics.append({\"frame\": frame_num, **metrics})\n",
    "\n",
    "        # print(\n",
    "        #     f\"Frame {frame_num:4d}: TP={metrics['tp']:2d} FP={metrics['fp']:2d} FN={metrics['fn']:2d} \"\n",
    "        #     f\"P={metrics['precision']:.3f} R={metrics['recall']:.3f} F1={metrics['f1']:.3f}\"\n",
    "        # )\n",
    "\n",
    "        # Save comparison image\n",
    "        visualize_iou_comparison(\n",
    "            video_path,\n",
    "            frame_num,\n",
    "            pred_boxes,\n",
    "            gt_boxes,\n",
    "            is_tp,\n",
    "            vis_dir,\n",
    "            prefix=\"task2\",\n",
    "            pred_class_ids=pred_labels,\n",
    "            gt_class_ids=gt_class_ids,\n",
    "        )\n",
    "\n",
    "    # Overall metrics\n",
    "    overall_metrics = calculate_metrics(total_tp, total_fp, total_fn)\n",
    "\n",
    "    print(f\"Total: TP={total_tp} FP={total_fp} FN={total_fn}\")\n",
    "    print(f\"  Precision: {overall_metrics['precision']:.4f}  Recall: {overall_metrics['recall']:.4f}  F1: {overall_metrics['f1']:.4f}  Accuracy: {overall_metrics['accuracy']:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    return {\"overall\": overall_metrics, \"per_frame\": frame_metrics}\n",
    "\n",
    "\n",
    "def evaluate_task3_face_detections(\n",
    "    video_path: str,\n",
    "    gt_dir: str = \"ground_truth_for_task3\",\n",
    "    iou_thresh: float = 0.5,\n",
    "    eval_frames: Optional[List[int]] = None,\n",
    "    all_detections: Optional[List[Dict[str, NDArray]]] = None,\n",
    ") -> Dict[str, Union[Dict[str, float], List[Dict[str, float]]]]:\n",
    "    \"\"\"\n",
    "    Check face detection results (Task 3)\n",
    "\n",
    "    Args:\n",
    "        video_path: Video file\n",
    "        gt_dir: Ground truth folder\n",
    "        iou_thresh: IoU threshold (default: 0.5)\n",
    "        eval_frames: Frames to check (default: EVAL_FRAMES)\n",
    "        all_detections: Object detections to get person boxes\n",
    "\n",
    "    Returns:\n",
    "        Overall metrics\n",
    "    \"\"\"\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    img_width = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    img_height = int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Use default if not provided\n",
    "    if eval_frames is None:\n",
    "        eval_frames = EVAL_FRAMES\n",
    "\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    frame_metrics = []\n",
    "\n",
    "    print(\"FACE DETECTION EVALUATION\")\n",
    "\n",
    "\n",
    "    # Make output folder\n",
    "    video_name = Path(video_path).stem.replace(\"2\", \"\")  # Remove task2 suffix\n",
    "    vis_dir = f\"evaluation_results/{video_name}_task3\"\n",
    "    os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "    for frame_num in eval_frames:\n",
    "        # Get frame\n",
    "        vid.set(cv2.CAP_PROP_POS_FRAMES, frame_num)\n",
    "        ret, frame = vid.read()\n",
    "\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Get person boxes from object detections\n",
    "        person_boxes = []\n",
    "        if all_detections is not None and frame_num < len(all_detections):\n",
    "            det = all_detections[frame_num]\n",
    "            # COCO class 1 = person\n",
    "            for i, label in enumerate(det[\"labels\"]):\n",
    "                if label == 1:\n",
    "                    person_boxes.append(det[\"boxes\"][i])\n",
    "\n",
    "        # Detect faces\n",
    "        face_detections = facial_recognition_with_cascades(\n",
    "            frame, person_boxes=person_boxes if person_boxes else None\n",
    "        )\n",
    "\n",
    "        # Convert to boxes\n",
    "        pred_boxes = []\n",
    "        for det in face_detections:\n",
    "            x, y, w, h = det[\"box\"]\n",
    "            pred_boxes.append([x, y, x + w, y + h])\n",
    "\n",
    "        pred_boxes = np.array(pred_boxes) if pred_boxes else np.array([]).reshape(0, 4)\n",
    "\n",
    "        # Class 5 = face\n",
    "        pred_class_ids = (\n",
    "            np.full(len(pred_boxes), 5) if len(pred_boxes) > 0 else np.array([])\n",
    "        )\n",
    "\n",
    "        # GT file\n",
    "        gt_file = Path(gt_dir) / f\"task1_frame_{frame_num}.txt\"\n",
    "\n",
    "        if not gt_file.exists():\n",
    "            print(f\"No GT for frame {frame_num}\")\n",
    "            continue\n",
    "\n",
    "        # Evaluate\n",
    "        metrics, is_tp, gt_boxes, gt_class_ids = evaluate_detections_for_frame(\n",
    "            pred_boxes,\n",
    "            gt_file,\n",
    "            img_width,\n",
    "            img_height,\n",
    "            iou_thresh,\n",
    "            pred_class_ids=pred_class_ids,\n",
    "        )\n",
    "\n",
    "        total_tp += metrics[\"tp\"]\n",
    "        total_fp += metrics[\"fp\"]\n",
    "        total_fn += metrics[\"fn\"]\n",
    "\n",
    "        frame_metrics.append({\"frame\": frame_num, **metrics})\n",
    "\n",
    "        # print(\n",
    "        #     f\"Frame {frame_num:4d}: TP={metrics['tp']:2d} FP={metrics['fp']:2d} FN={metrics['fn']:2d} \"\n",
    "        #     f\"P={metrics['precision']:.3f} R={metrics['recall']:.3f} F1={metrics['f1']:.3f}\"\n",
    "        # )\n",
    "\n",
    "        # Save comparison\n",
    "        visualize_iou_comparison(\n",
    "            f\"{video_name}1.mp4\",\n",
    "            frame_num,\n",
    "            pred_boxes,\n",
    "            gt_boxes,\n",
    "            is_tp,\n",
    "            vis_dir,\n",
    "            prefix=\"task3\",\n",
    "            pred_class_ids=pred_class_ids,\n",
    "            gt_class_ids=gt_class_ids,\n",
    "        )\n",
    "\n",
    "    vid.release()\n",
    "\n",
    "    # Overall metrics\n",
    "    overall_metrics = calculate_metrics(total_tp, total_fp, total_fn)\n",
    "\n",
    "    print(f\"Total: TP={total_tp} FP={total_fp} FN={total_fn}\")\n",
    "    print(f\"  Precision: {overall_metrics['precision']:.4f}  Recall: {overall_metrics['recall']:.4f}  F1: {overall_metrics['f1']:.4f}  Accuracy: {overall_metrics['accuracy']:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "    return {\"overall\": overall_metrics, \"per_frame\": frame_metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WF13XL_kA2IS"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Export frames for Ground Truth annotation\n",
    "# video_name = \"task\"\n",
    "# FOR_TASK = \"1\"\n",
    "# export_specific_frames(\n",
    "#     video_path=(f\"{video_name}{FOR_TASK}.mp4\"),\n",
    "#     output_dir=(f\"ground_truth_frames/{video_name}{FOR_TASK}\"),\n",
    "#     frame_numbers=EVAL_FRAMES,\n",
    "#     prefix=f\"{video_name.strip('_')}{FOR_TASK}_frame\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTgyQGjvAJaW"
   },
   "outputs": [],
   "source": [
    "## Run parameters ##\n",
    "video_name = \"task\"\n",
    "evaluate = True\n",
    "eval_frames = None\n",
    "\n",
    "## DETECTION PARAMETERS OVERRIDES ##\n",
    "# MaskRCNN settings\n",
    "RCNN_SCORE_THRESHOLD = 0.7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gusHeB7Foz6t"
   },
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGojvHOfBbbA"
   },
   "source": [
    "### Task 2 - Object detection and tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TV2OMHV0AvGg",
    "outputId": "ff088cfd-6e69-4f06-90ac-1d519344b6d0"
   },
   "outputs": [],
   "source": [
    "# Tracking method\n",
    "association_method = \"centroid_distance\"  # or 'iou'\n",
    "\n",
    "video_path = f\"./{video_name}1.mp4\"\n",
    "\n",
    "all_object_detections = object_detections(video_path, video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c_pBYQQwoz6t"
   },
   "outputs": [],
   "source": [
    "print(f\"Object Tracking Method: {association_method}\")\n",
    "all_tracks = do_tracking(\n",
    "    all_object_detections, association_method=association_method\n",
    ")\n",
    "\n",
    "# NOTE: Track IDs are already consecutive (0, 1, 2, ...)\n",
    "# They come from enumerate() in draw_tracks_on_video()\n",
    "print(f\"Total tracks after filtering: {len(all_tracks)}\")\n",
    "\n",
    "print(\"\\nMaking tracked video...\")\n",
    "draw_tracks_on_video(video_path, all_tracks, video_name=video_name)\n",
    "\n",
    "print(\"\\nTask 2 done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgKFgF4wBgQn"
   },
   "source": [
    "### Task 2 - Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C1giyZdCVh_"
   },
   "outputs": [],
   "source": [
    "# Evaluate Task 2 object detections with ground truth\n",
    "if evaluate and Path(\"ground_truth_for_task2\").exists():\n",
    "    evaluate_task2_detections(\n",
    "        all_object_detections,\n",
    "        video_name,\n",
    "        gt_dir=\"ground_truth_for_task2\",\n",
    "        iou_thresh=0.5,\n",
    "        eval_frames=eval_frames,\n",
    "    )\n",
    "\n",
    "# Evaluate track quality\n",
    "print(\"OBJECT TRACKING EVALUATION\")\n",
    "track_quality = evaluate_track_quality(all_tracks, fps=30)\n",
    "print(f\"Total Tracks:              {track_quality['total_tracks']}\")\n",
    "print(f\"Average Track Length:      {track_quality['avg_track_length_frames']:.1f} frames ({track_quality['avg_track_length_seconds']:.2f}s)\")\n",
    "print(f\"Median Track Length:       {track_quality['median_track_length_frames']:.0f} frames ({track_quality['median_track_length_seconds']:.2f}s)\")\n",
    "\n",
    "\n",
    "# Evaluate tracking accuracy against ground truth\n",
    "print(\"\\nTracking Accuracy (vs Ground Truth):\")\n",
    "tracking_acc = evaluate_tracking_accuracy(\n",
    "    detected_tracks=track_quality['total_tracks'],\n",
    "    ground_truth_tracks=GT_DIFFERENT_TRACKS_IN_VIDEO\n",
    ")\n",
    "print(f\"  Precision: {tracking_acc['precision']:.4f}  Recall: {tracking_acc['recall']:.4f}  F1: {tracking_acc['f1']:.4f}  Accuracy: {tracking_acc['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOr5WqTnoz6t"
   },
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4oqOjR0CdzX"
   },
   "source": [
    "### Task 3 - Face detection and tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fv4zIqUuoz6t"
   },
   "outputs": [],
   "source": [
    "## DETECTION PARAMETERS OVERRIDES ##\n",
    "# Haar cascade settings\n",
    "SCALE_FACTOR = 1.1\n",
    "MIN_NEIGHBORS = 3\n",
    "MIN_SIZE = (30, 30)\n",
    "MAX_SIZE = (100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8XlYNkTCsyd"
   },
   "outputs": [],
   "source": [
    "# Task 3\n",
    "print(\"\\nTask 3: Faces detection and tracking...\")\n",
    "input_video_detection_path = f\"{video_name}1.mp4\"\n",
    "video_task2_path = f\"{video_name}2.mp4\"\n",
    "output_video_path = f\"{video_name}3.mp4\"\n",
    "\n",
    "\n",
    "current_video = cv2.VideoCapture(input_video_detection_path)\n",
    "video_length = int(current_video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fps = int(current_video.get(cv2.CAP_PROP_FPS))\n",
    "width = int(current_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(current_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# First pass: collect all face detections\n",
    "print(\"Detecting faces in all frames...\")\n",
    "all_face_detections = []\n",
    "\n",
    "for i in range(video_length):\n",
    "    # Read frame\n",
    "    ret, frame = current_video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get person boxes for this frame\n",
    "    person_boxes = []\n",
    "    if all_object_detections is not None and i < len(all_object_detections):\n",
    "        det = all_object_detections[i]\n",
    "        # COCO class 1 = person\n",
    "        for idx, label in enumerate(det[\"labels\"]):\n",
    "            if label == 1:\n",
    "                person_boxes.append(det[\"boxes\"][idx])\n",
    "\n",
    "    # Detect faces\n",
    "    face_detections = facial_recognition_with_cascades(\n",
    "        frame, person_boxes=person_boxes if person_boxes else None\n",
    "    )\n",
    "\n",
    "    all_face_detections.append(face_detections)\n",
    "\n",
    "current_video.release()\n",
    "print(f\"Detected faces in {video_length} frames\")\n",
    "\n",
    "# Track faces across frames\n",
    "print(\"Tracking faces...\")\n",
    "face_tracks = do_face_tracking(all_face_detections, association_method=\"centroid_distance\")\n",
    "print(f\"Created {len(face_tracks)} face tracks\")\n",
    "\n",
    "# Second pass: draw tracked faces on video\n",
    "print(\"\\nDrawing tracked faces on video...\")\n",
    "current_video = cv2.VideoCapture(video_task2_path)\n",
    "fourcc = cv2.VideoWriter_fourcc(\"m\", \"p\", \"4\", \"v\")\n",
    "vid_out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "for i in range(video_length):\n",
    "    # Read frame\n",
    "    ret, frame = current_video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Draw all active face tracks\n",
    "    for track_id, track in enumerate(face_tracks):\n",
    "        # Check if track is active on this frame\n",
    "        start_frame = track[\"start_frame\"]\n",
    "        inner_idx = i - start_frame\n",
    "\n",
    "        if 0 <= inner_idx < len(track[\"boxes\"]):\n",
    "            # Get box [tlx, tly, brx, bry]\n",
    "            tlx, tly, brx, bry = track[\"boxes\"][inner_idx].astype(np.int32)\n",
    "            cascade_name = track[\"cascade_names\"][inner_idx]\n",
    "\n",
    "            # Color by track ID\n",
    "            colour = COLOURS2[track_id % len(COLOURS)]\n",
    "\n",
    "            # Draw box\n",
    "            cv2.rectangle(frame, (tlx, tly), (brx, bry), color=colour, thickness=2)\n",
    "\n",
    "            # Make label\n",
    "            text = f\"ID:{track_id} face:{cascade_name}\"\n",
    "            (text_width, text_height), _ = cv2.getTextSize(\n",
    "                text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1\n",
    "            )\n",
    "\n",
    "            # Background\n",
    "            cv2.rectangle(\n",
    "                frame,\n",
    "                (tlx, tly - text_height - 4),\n",
    "                (tlx + text_width, tly),\n",
    "                colour,\n",
    "                -1,\n",
    "            )\n",
    "\n",
    "            # Text\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                text,\n",
    "                (tlx, tly - 2),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.5,\n",
    "                (255, 255, 255),\n",
    "                1,\n",
    "                cv2.LINE_AA,\n",
    "            )\n",
    "\n",
    "    vid_out.write(frame)\n",
    "\n",
    "current_video.release()\n",
    "vid_out.release()\n",
    "print(f\"Face tracking video saved to {output_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyZu3pRACyg1"
   },
   "source": [
    "### Task 3 - Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5CyAY-mCyAp"
   },
   "outputs": [],
   "source": [
    "# Evaluate Task 3\n",
    "if evaluate and Path(\"ground_truth_for_task3\").exists():\n",
    "    evaluate_task3_face_detections(\n",
    "        video_task2_path,\n",
    "        gt_dir=\"ground_truth_for_task3\",\n",
    "        iou_thresh=0.5,\n",
    "        eval_frames=eval_frames,\n",
    "        all_detections=all_object_detections,\n",
    "    )\n",
    "\n",
    "\n",
    "# Evaluate face track quality\n",
    "print(\"FACE TRACKING QUALITY EVALUATION\")\n",
    "face_track_quality = evaluate_track_quality(face_tracks, fps=fps)\n",
    "print(f\"Total Tracks:              {face_track_quality['total_tracks']}\")\n",
    "print(f\"Average Track Length:      {face_track_quality['avg_track_length_frames']:.1f} frames ({face_track_quality['avg_track_length_seconds']:.2f}s)\")\n",
    "print(f\"Median Track Length:       {face_track_quality['median_track_length_frames']:.0f} frames ({face_track_quality['median_track_length_seconds']:.2f}s)\")\n",
    "\n",
    "# Evaluate face tracking accuracy against ground truth\n",
    "print(\"\\nFace Tracking Accuracy (vs Ground Truth):\")\n",
    "face_tracking_acc = evaluate_tracking_accuracy(\n",
    "    detected_tracks=face_track_quality['total_tracks'],\n",
    "    ground_truth_tracks=GT_FACES_TRACKS_IN_VIDEO\n",
    ")\n",
    "print(f\"  Precision: {face_tracking_acc['precision']:.4f}  Recall: {face_tracking_acc['recall']:.4f}  F1: {face_tracking_acc['f1']:.4f}  Accuracy: {face_tracking_acc['accuracy']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "history_visible": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
